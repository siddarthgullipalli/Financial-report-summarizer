{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Report Summarizer with ChromaDB and HuggingFace EDGAR Corpus\n",
    "\n",
    "This notebook implements a complete RAG (Retrieval-Augmented Generation) system for financial document analysis using:\n",
    "- **ChromaDB** for persistent vector storage (instead of FAISS)\n",
    "- **HuggingFace EDGAR Corpus** dataset (real SEC filings)\n",
    "- **FinBERT** embeddings optimized for financial text\n",
    "- **Advanced techniques**: Hybrid search, re-ranking, few-shot prompting\n",
    "\n",
    "## Features\n",
    "1. ‚úÖ Persistent ChromaDB vector database\n",
    "2. ‚úÖ Real EDGAR corpus from HuggingFace\n",
    "3. ‚úÖ Hybrid search (semantic + keyword)\n",
    "4. ‚úÖ Cross-encoder re-ranking\n",
    "5. ‚úÖ Few-shot prompting\n",
    "6. ‚úÖ GPU-accelerated embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Install ALL packages with correct versions\n\n!pip install -q sentence-transformers==2.2.2\n!pip install -q transformers==4.35.2\n!pip install -q datasets==2.14.6\n!pip install -q openai==1.12.0\n!pip install -q pypdf==3.17.1\n!pip install -q chromadb==0.4.18\n!pip install -q torch torchvision torchaudio\n\nprint(\"‚úÖ All packages installed!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configure OpenAI API Key\n",
    "\n",
    "import os\n",
    "\n",
    "# Set your OpenAI API key here\n",
    "os.environ['OPENAI_API_KEY'] = 'your-api-key-here'  # Replace with your actual key\n",
    "\n",
    "# Or if running in Colab, you can use this:\n",
    "# from google.colab import userdata\n",
    "# os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "print(\"‚úÖ API key configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä ChromaDB-Powered RAG System\n",
    "\n",
    "This implementation uses **ChromaDB** for persistent vector storage instead of FAISS.\n",
    "\n",
    "### Key Advantages:\n",
    "- **Persistent**: Data survives notebook restarts\n",
    "- **Scalable**: Handles millions of documents efficiently\n",
    "- **Metadata filtering**: Can filter by company, date, section, etc.\n",
    "- **No manual indexing**: Automatically indexes on insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Complete FinBERT-powered RAG System with ChromaDB\n",
    "\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import numpy as np\n",
    "import pypdf\n",
    "from openai import OpenAI\n",
    "from typing import Optional, List, Dict\n",
    "import re\n",
    "\n",
    "class FinBERTFinancialRAG:\n",
    "    \"\"\"\n",
    "    Complete Financial RAG System with ChromaDB:\n",
    "    - FinBERT embeddings for financial text understanding\n",
    "    - ChromaDB for persistent vector storage\n",
    "    - Support for EDGAR corpus\n",
    "    - Support for PDF uploads\n",
    "    - GPU acceleration for embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, use_finbert: bool = True, persist_directory: str = None):\n",
    "        \"\"\"\n",
    "        Initialize the system\n",
    "\n",
    "        Args:\n",
    "            use_finbert: If True, use FinBERT. If False, use faster model.\n",
    "            persist_directory: Where to store ChromaDB data\n",
    "        \"\"\"\n",
    "        print(\"ü§ñ Initializing Financial RAG System with ChromaDB...\")\n",
    "\n",
    "        # Set up persistent storage\n",
    "        if persist_directory is None:\n",
    "            persist_directory = os.path.expanduser(\"~/FinancialAI/chromadb\")\n",
    "\n",
    "        os.makedirs(persist_directory, exist_ok=True)\n",
    "        print(f\"üìÅ Database location: {persist_directory}\")\n",
    "\n",
    "        # Initialize ChromaDB client\n",
    "        self.chroma_client = chromadb.PersistentClient(path=persist_directory)\n",
    "\n",
    "        # Choose embedding model\n",
    "        if use_finbert:\n",
    "            model_name = \"ProsusAI/finbert\"\n",
    "            print(f\"  üìä Loading FinBERT (optimized for finance)...\")\n",
    "        else:\n",
    "            model_name = \"all-MiniLM-L6-v2\"\n",
    "            print(f\"  üìä Loading Sentence Transformer (faster)...\")\n",
    "\n",
    "        self.embedder = SentenceTransformer(model_name)\n",
    "        self.embedding_dim = self.embedder.get_sentence_embedding_dimension()\n",
    "\n",
    "        print(f\"  ‚úÖ Model loaded! Embedding dimension: {self.embedding_dim}\")\n",
    "\n",
    "        # Create or get collection\n",
    "        collection_name = \"financial_filings\"\n",
    "\n",
    "        try:\n",
    "            # Try to get existing collection\n",
    "            self.collection = self.chroma_client.get_collection(name=collection_name)\n",
    "            print(f\"  ‚úÖ Loaded existing collection: {collection_name}\")\n",
    "            print(f\"  üìä Documents in collection: {self.collection.count()}\")\n",
    "\n",
    "            # Rebuild chunks and metadata from existing collection\n",
    "            self._rebuild_chunks_from_collection()\n",
    "\n",
    "        except:\n",
    "            # Create new collection\n",
    "            self.collection = self.chroma_client.create_collection(\n",
    "                name=collection_name,\n",
    "                metadata={\"description\": \"Financial SEC filings with FinBERT embeddings\"}\n",
    "            )\n",
    "            print(f\"  ‚úÖ Created new collection: {collection_name}\")\n",
    "\n",
    "            # Initialize empty lists\n",
    "            self.chunks = []\n",
    "            self.chunk_metadata = []\n",
    "\n",
    "        # Initialize OpenAI\n",
    "        api_key = os.getenv('OPENAI_API_KEY')\n",
    "        if api_key:\n",
    "            self.client = OpenAI(api_key=api_key)\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è  OpenAI API key not set - you'll need to set it before asking questions\")\n",
    "            self.client = None\n",
    "\n",
    "        self.documents_loaded = []\n",
    "\n",
    "        print(\"‚úÖ System ready!\\n\")\n",
    "\n",
    "    def _rebuild_chunks_from_collection(self):\n",
    "        \"\"\"Rebuild chunks and metadata lists from ChromaDB collection\"\"\"\n",
    "        if self.collection.count() == 0:\n",
    "            self.chunks = []\n",
    "            self.chunk_metadata = []\n",
    "            return\n",
    "\n",
    "        # Get all documents from collection\n",
    "        results = self.collection.get()\n",
    "\n",
    "        # Rebuild chunks and metadata\n",
    "        self.chunks = results['documents']\n",
    "        self.chunk_metadata = results['metadatas']\n",
    "\n",
    "        print(f\"  üì¶ Loaded {len(self.chunks)} chunks from collection\")\n",
    "\n",
    "    def load_from_edgar(self, document):\n",
    "        \"\"\"\n",
    "        Load a document from EDGAR corpus\n",
    "\n",
    "        Args:\n",
    "            document: Single row from EDGAR dataset\n",
    "        \"\"\"\n",
    "        company = document['company']\n",
    "        filing_type = document['filing_type']\n",
    "        filing_date = document['filing_date']\n",
    "\n",
    "        print(f\"üìÑ Loading: {company} - {filing_type} ({filing_date})\")\n",
    "\n",
    "        # Extract available sections\n",
    "        sections_data = []\n",
    "\n",
    "        if document.get('item_1'):\n",
    "            sections_data.append(('Item 1 - Business', document['item_1']))\n",
    "\n",
    "        if document.get('item_1a'):\n",
    "            sections_data.append(('Item 1A - Risk Factors', document['item_1a']))\n",
    "\n",
    "        if document.get('item_7'):\n",
    "            sections_data.append(('Item 7 - MD&A', document['item_7']))\n",
    "\n",
    "        if document.get('item_7a'):\n",
    "            sections_data.append(('Item 7A - Quantitative Disclosures', document['item_7a']))\n",
    "\n",
    "        print(f\"  üìë Found {len(sections_data)} sections\")\n",
    "\n",
    "        # Chunk each section\n",
    "        all_chunks = []\n",
    "        all_metadatas = []\n",
    "        all_ids = []\n",
    "        doc_chunk_count = 0\n",
    "\n",
    "        for section_name, section_text in sections_data:\n",
    "            chunks = self._chunk_text(section_text)\n",
    "\n",
    "            for chunk in chunks:\n",
    "                # Create unique ID\n",
    "                doc_id = f\"{company}_{filing_date}_{section_name}_{doc_chunk_count}\"\n",
    "                doc_id = doc_id.replace(' ', '_').replace('/', '_').replace('-', '_')\n",
    "\n",
    "                all_chunks.append(chunk)\n",
    "                all_metadatas.append({\n",
    "                    'company': company,\n",
    "                    'filing_type': filing_type,\n",
    "                    'filing_date': filing_date,\n",
    "                    'section': section_name,\n",
    "                    'source': 'EDGAR'\n",
    "                })\n",
    "                all_ids.append(doc_id)\n",
    "                doc_chunk_count += 1\n",
    "\n",
    "        # Generate embeddings\n",
    "        print(f\"  üßÆ Generating embeddings for {len(all_chunks)} chunks...\")\n",
    "        embeddings = self.embedder.encode(\n",
    "            all_chunks,\n",
    "            show_progress_bar=False,\n",
    "            batch_size=32,\n",
    "            convert_to_numpy=True,\n",
    "            device='cuda'\n",
    "        )\n",
    "\n",
    "        # Add to ChromaDB\n",
    "        self.collection.add(\n",
    "            embeddings=embeddings.tolist(),\n",
    "            documents=all_chunks,\n",
    "            metadatas=all_metadatas,\n",
    "            ids=all_ids\n",
    "        )\n",
    "\n",
    "        # Also add to local lists for backward compatibility with HybridSearch\n",
    "        self.chunks.extend(all_chunks)\n",
    "        self.chunk_metadata.extend(all_metadatas)\n",
    "\n",
    "        # Track document\n",
    "        self.documents_loaded.append({\n",
    "            'company': company,\n",
    "            'filing_type': filing_type,\n",
    "            'filing_date': filing_date,\n",
    "            'source': 'EDGAR',\n",
    "            'chunks': doc_chunk_count\n",
    "        })\n",
    "\n",
    "        print(f\"  ‚úÖ Added {doc_chunk_count} chunks to ChromaDB\")\n",
    "        print(f\"  üìä Total documents in DB: {self.collection.count()}\\n\")\n",
    "\n",
    "    def load_from_pdf(self, pdf_path: str, company_name: str):\n",
    "        \"\"\"\n",
    "        Load and process a PDF file\n",
    "\n",
    "        Args:\n",
    "            pdf_path: Path to PDF file\n",
    "            company_name: Name of company\n",
    "        \"\"\"\n",
    "        print(f\"üìÑ Loading PDF: {pdf_path}\")\n",
    "\n",
    "        # Extract text from PDF\n",
    "        text = \"\"\n",
    "        try:\n",
    "            reader = pypdf.PdfReader(pdf_path)\n",
    "            total_pages = len(reader.pages)\n",
    "            print(f\"  üìñ Extracting text from {total_pages} pages...\")\n",
    "\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text()\n",
    "\n",
    "            print(f\"  ‚úÖ Extracted {len(text)} characters\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error reading PDF: {e}\")\n",
    "            return\n",
    "\n",
    "        # Chunk the text\n",
    "        chunks = self._chunk_text(text)\n",
    "        print(f\"  ‚úÇÔ∏è  Created {len(chunks)} chunks\")\n",
    "\n",
    "        # Generate embeddings\n",
    "        print(f\"  üßÆ Generating embeddings...\")\n",
    "        embeddings = self.embedder.encode(\n",
    "            chunks,\n",
    "            show_progress_bar=True,\n",
    "            batch_size=32,\n",
    "            convert_to_numpy=True,\n",
    "            device='cuda'\n",
    "        )\n",
    "\n",
    "        # Prepare for ChromaDB\n",
    "        all_ids = []\n",
    "        all_metadatas = []\n",
    "\n",
    "        for i in range(len(chunks)):\n",
    "            doc_id = f\"{company_name}_PDF_{i}\"\n",
    "            doc_id = doc_id.replace(' ', '_').replace('/', '_')\n",
    "\n",
    "            all_ids.append(doc_id)\n",
    "            all_metadatas.append({\n",
    "                'company': company_name,\n",
    "                'filing_type': 'PDF Upload',\n",
    "                'filing_date': 'N/A',\n",
    "                'section': 'PDF Document',\n",
    "                'source': 'PDF'\n",
    "            })\n",
    "\n",
    "        # Add to ChromaDB\n",
    "        self.collection.add(\n",
    "            embeddings=embeddings.tolist(),\n",
    "            documents=chunks,\n",
    "            metadatas=all_metadatas,\n",
    "            ids=all_ids\n",
    "        )\n",
    "\n",
    "        # Add to local lists\n",
    "        self.chunks.extend(chunks)\n",
    "        self.chunk_metadata.extend(all_metadatas)\n",
    "\n",
    "        # Track document\n",
    "        self.documents_loaded.append({\n",
    "            'company': company_name,\n",
    "            'filing_type': 'PDF Upload',\n",
    "            'filing_date': 'N/A',\n",
    "            'source': 'PDF',\n",
    "            'chunks': len(chunks)\n",
    "        })\n",
    "\n",
    "        print(f\"  ‚úÖ Added {len(chunks)} chunks to ChromaDB\")\n",
    "        print(f\"  üìä Total documents in DB: {self.collection.count()}\\n\")\n",
    "\n",
    "    def _chunk_text(self, text: str, chunk_size: int = 500):\n",
    "        \"\"\"\n",
    "        Split text into chunks\n",
    "\n",
    "        Args:\n",
    "            text: Text to chunk\n",
    "            chunk_size: Target size for each chunk\n",
    "\n",
    "        Returns:\n",
    "            List of text chunks\n",
    "        \"\"\"\n",
    "        # Split into paragraphs\n",
    "        paragraphs = text.split('\\n\\n')\n",
    "\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "\n",
    "        for para in paragraphs:\n",
    "            para = para.strip()\n",
    "\n",
    "            # Skip very short paragraphs\n",
    "            if len(para) < 50:\n",
    "                continue\n",
    "\n",
    "            # If adding this paragraph exceeds chunk_size, save current chunk\n",
    "            if len(current_chunk) + len(para) > chunk_size and current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = para\n",
    "            else:\n",
    "                # Add to current chunk\n",
    "                current_chunk += \"\\n\\n\" + para if current_chunk else para\n",
    "\n",
    "        # Add final chunk\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def build_index(self, use_gpu: bool = False):\n",
    "        \"\"\"\n",
    "        Build index - for ChromaDB this is a no-op as indexing happens automatically\n",
    "        This method exists for backward compatibility with the FAISS version\n",
    "\n",
    "        Args:\n",
    "            use_gpu: Ignored for ChromaDB\n",
    "        \"\"\"\n",
    "        print(\"‚ÑπÔ∏è  ChromaDB indexes automatically - no manual build needed!\")\n",
    "        print(f\"‚úÖ Collection ready with {self.collection.count()} documents\")\n",
    "\n",
    "    def ask(self, question: str, top_k: int = 5):\n",
    "        \"\"\"\n",
    "        Ask a question about the documents\n",
    "\n",
    "        Args:\n",
    "            question: Question to ask\n",
    "            top_k: Number of relevant chunks to retrieve\n",
    "\n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        if self.collection.count() == 0:\n",
    "            print(\"‚ùå No documents loaded! Please load documents first with build_index()\")\n",
    "            return None\n",
    "\n",
    "        if self.client is None:\n",
    "            print(\"‚ùå OpenAI API key not set!\")\n",
    "            return None\n",
    "\n",
    "        print(f\"‚ùì Question: {question}\\n\")\n",
    "        print(\"  üîç Searching ChromaDB for relevant information...\")\n",
    "\n",
    "        # Generate question embedding\n",
    "        q_embedding = self.embedder.encode([question], device='cuda')\n",
    "\n",
    "        # Query ChromaDB\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=q_embedding.tolist(),\n",
    "            n_results=top_k\n",
    "        )\n",
    "\n",
    "        # Extract results\n",
    "        chunks = results['documents'][0]\n",
    "        metadatas = results['metadatas'][0]\n",
    "\n",
    "        # Build context\n",
    "        context_parts = []\n",
    "        sources_used = []\n",
    "\n",
    "        for i, (chunk, meta) in enumerate(zip(chunks, metadatas)):\n",
    "            source_info = f\"{meta['company']} | {meta['section']}\"\n",
    "            sources_used.append(source_info)\n",
    "            context_parts.append(f\"[Source {i+1}: {source_info}]\\n{chunk}\")\n",
    "\n",
    "        context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "\n",
    "        # Generate answer\n",
    "        prompt = f\"\"\"You are an expert financial analyst with deep knowledge of SEC filings and financial statements.\n",
    "\n",
    "Context from financial documents:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "1. Answer ONLY using information from the context above\n",
    "2. Think step-by-step if calculations are needed\n",
    "3. Always cite which source (company name and section) you're using\n",
    "4. Show your work for any calculations or comparisons\n",
    "5. Be precise with numbers and units (millions, billions, percentages)\n",
    "6. If information is not in the context, explicitly state \"Information not available\"\n",
    "\n",
    "Your analysis:\"\"\"\n",
    "\n",
    "        print(\"  ü§î Generating answer with GPT-3.5-turbo...\")\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert financial analyst.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.3,\n",
    "                max_tokens=800\n",
    "            )\n",
    "\n",
    "            answer = response.choices[0].message.content\n",
    "\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"üìä ANSWER\")\n",
    "            print(\"=\"*70)\n",
    "            print(answer)\n",
    "            print(\"=\"*70)\n",
    "\n",
    "            print(\"\\nüìö Sources Used:\")\n",
    "            for i, source in enumerate(set(sources_used), 1):\n",
    "                print(f\"  {i}. {source}\")\n",
    "            print()\n",
    "\n",
    "            return answer\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error generating answer: {e}\")\n",
    "            return None\n",
    "\n",
    "    def list_documents(self):\n",
    "        \"\"\"Show all loaded documents\"\"\"\n",
    "\n",
    "        if not self.documents_loaded:\n",
    "            print(\"üì≠ No documents loaded yet\")\n",
    "            return\n",
    "\n",
    "        print(f\"üìö Loaded Documents ({len(self.documents_loaded)}):\n",
    "\")\n",
    "\n",
    "        for i, doc in enumerate(self.documents_loaded, 1):\n",
    "            print(f\"{i}. {doc['company']}\")\n",
    "            print(f\"   Type: {doc['filing_type']}\")\n",
    "            print(f\"   Source: {doc['source']}\")\n",
    "            print(f\"   Date: {doc.get('filing_date', 'N/A')}\")\n",
    "            print(f\"   Chunks: {doc['chunks']}\")\n",
    "            print()\n",
    "\n",
    "    def delete_collection(self):\n",
    "        \"\"\"Delete the entire collection\"\"\"\n",
    "        self.chroma_client.delete_collection(name=\"financial_filings\")\n",
    "        self.chunks = []\n",
    "        self.chunk_metadata = []\n",
    "        self.documents_loaded = []\n",
    "        print(\"üóëÔ∏è  Collection deleted!\")\n",
    "\n",
    "\n",
    "# Initialize the system\n",
    "print(\"=\"*70)\n",
    "print(\"   üöÄ FINANCIAL AI SYSTEM - CHROMADB + FINBERT\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "rag = FinBERTFinancialRAG(use_finbert=True)\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Load Real EDGAR Corpus from HuggingFace\n",
    "\n",
    "We're using the `eloukas/edgar-corpus` dataset which contains real SEC filings from public companies.\n",
    "\n",
    "**Dataset Features:**\n",
    "- Real 10-K filings from S&P 500 companies\n",
    "- Multiple sections: Item 1, 1A, 7, 7A\n",
    "- Structured data with company names, dates, and filing types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Load REAL EDGAR Corpus from HuggingFace\n\nfrom datasets import load_dataset\nimport time\n\nprint(\"=\"*70)\nprint(\"   üì• LOADING EDGAR CORPUS FROM HUGGINGFACE\")\nprint(\"=\"*70)\nprint()\n\nstart_time = time.time()\n\n# Load dataset - Final model configuration\n# Total available: 20,000+ companies in EDGAR corpus\n# Using 500 companies for batch processing demonstration\nNUM_COMPANIES = 500\n\nprint(f\"üåê Loading {NUM_COMPANIES} companies from edgar-corpus...\")\nprint(\"‚è≥ This may take a few minutes on first load...\\n\")\n\ndataset = load_dataset(\n    \"eloukas/edgar-corpus\",\n    split=f\"train[:{NUM_COMPANIES}]\"\n)\n\nelapsed = time.time() - start_time\n\nprint(f\"‚úÖ Loaded {len(dataset)} companies in {elapsed:.2f} seconds\\n\")\n\n# DIAGNOSTIC: Check dataset structure\nprint(\"=\"*70)\nprint(\"   üîç DATASET STRUCTURE DIAGNOSTIC\")\nprint(\"=\"*70)\n\nif len(dataset) > 0:\n    first_item = dataset[0]\n    print(f\"\\nüìä Available fields in dataset:\")\n    for key in first_item.keys():\n        value = first_item[key]\n        if isinstance(value, str):\n            preview = value[:100] + \"...\" if len(value) > 100 else value\n        else:\n            preview = str(value)\n        print(f\"  ‚Ä¢ {key}: {type(value).__name__} - {preview}\")\n    \n    print(f\"\\nüìä Item fields: {[k for k in first_item.keys() if k.startswith('item_')]}\")\nelse:\n    print(\"‚ùå Dataset is empty!\")\n\nprint(\"\\n\" + \"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Load documents from EDGAR corpus into ChromaDB with GPU-Optimized Batching\n\nimport time\n\n# =============================================================================\n# HELPER FUNCTION FOR EMBEDDING GENERATION\n# =============================================================================\n\ndef get_finbert_embedding(text):\n    \"\"\"Generate FinBERT embedding for a single text\"\"\"\n    embedding = rag.embedder.encode(\n        [text],\n        show_progress_bar=False,\n        convert_to_numpy=True,\n        device='cuda'\n    )\n    return embedding[0]\n\n# =============================================================================\n# PROCESS WITH BATCHING (FASTER ON GPU)\n# =============================================================================\n\nprint(\"=\"*70)\nprint(\"   üì§ LOADING DOCUMENTS INTO CHROMADB (BATCHED)\")\nprint(\"=\"*70)\nprint()\n\nprint(f\"‚öôÔ∏è  Processing {NUM_COMPANIES} companies with GPU batching...\\n\")\nstart_time = time.time()\n\n# Batch processing for GPU efficiency\nBATCH_SIZE = 8  # Process 8 companies at once on GPU\nbatch_texts = []\nbatch_metadatas = []\nbatch_ids = []\nprocessed_count = 0\nerror_count = 0\n\n# Define which sections to process (most important financial sections)\nSECTIONS_TO_PROCESS = [\n    ('section_1', 'Business Description'),\n    ('section_7', 'MD&A'),\n    ('section_8', 'Financial Statements')\n]\n\nfor idx, company_data in enumerate(dataset):\n    if idx >= NUM_COMPANIES:\n        break\n    \n    try:\n        # Extract metadata fields - use actual field names from dataset\n        cik = company_data.get('cik', f'Unknown_{idx}')\n        year = company_data.get('year', 'Unknown')\n        filename = company_data.get('filename', f'doc_{idx}.txt')\n        \n        # Process each relevant section\n        for section_field, section_name in SECTIONS_TO_PROCESS:\n            section_text = company_data.get(section_field, '')\n            \n            # Validate text content\n            if not section_text or len(section_text.strip()) < 100:\n                continue\n            \n            # Take sample (first 3000 characters for faster processing)\n            text_sample = section_text[:3000]\n            \n            # Add to batch\n            batch_texts.append(text_sample)\n            batch_ids.append(f\"cik_{cik}_{year}_{section_field}_{len(batch_texts)}\")\n            batch_metadatas.append({\n                \"company\": f\"CIK {cik}\",\n                \"filing_type\": \"10-K\",\n                \"filing_date\": str(year),\n                \"section\": section_name,\n                \"cik\": cik,\n                \"year\": year,\n                \"index\": idx\n            })\n            \n            # Process batch when full\n            if len(batch_texts) >= BATCH_SIZE:\n                try:\n                    # Create embeddings for batch (GPU accelerated)\n                    embeddings = rag.embedder.encode(\n                        batch_texts,\n                        show_progress_bar=False,\n                        batch_size=BATCH_SIZE,\n                        convert_to_numpy=True,\n                        device='cuda'\n                    )\n                    \n                    # Add to ChromaDB\n                    rag.collection.add(\n                        documents=batch_texts,\n                        embeddings=embeddings.tolist(),\n                        ids=batch_ids,\n                        metadatas=batch_metadatas\n                    )\n                    \n                    # Update local tracking\n                    rag.chunks.extend(batch_texts)\n                    rag.chunk_metadata.extend(batch_metadatas)\n                    \n                    processed_count += len(batch_texts)\n                    \n                    # Progress update every 50 documents\n                    if processed_count % 50 == 0:\n                        elapsed = time.time() - start_time\n                        rate = processed_count / elapsed\n                        print(f\"   ‚úì {processed_count} chunks | {rate:.1f} chunks/sec | {elapsed:.1f}s elapsed\")\n                \n                except Exception as batch_error:\n                    print(f\"‚ùå Batch error at doc {idx}: {batch_error}\")\n                    error_count += len(batch_texts)\n                \n                # Clear batch\n                batch_texts = []\n                batch_ids = []\n                batch_metadatas = []\n    \n    except Exception as e:\n        print(f\"‚ùå Error at {idx}: {str(e)[:50]}\")\n        error_count += 1\n        continue\n\n# Process remaining batch\nif batch_texts:\n    try:\n        embeddings = rag.embedder.encode(\n            batch_texts,\n            show_progress_bar=False,\n            batch_size=len(batch_texts),\n            convert_to_numpy=True,\n            device='cuda'\n        )\n        \n        rag.collection.add(\n            documents=batch_texts,\n            embeddings=embeddings.tolist(),\n            ids=batch_ids,\n            metadatas=batch_metadatas\n        )\n        \n        rag.chunks.extend(batch_texts)\n        rag.chunk_metadata.extend(batch_metadatas)\n        \n        processed_count += len(batch_texts)\n    except Exception as e:\n        print(f\"‚ùå Final batch error: {e}\")\n        error_count += len(batch_texts)\n\ntotal_time = time.time() - start_time\n\n# =============================================================================\n# SUMMARY\n# =============================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"   ‚úÖ LOADING COMPLETE\")\nprint(\"=\"*70)\n\nprint(f\"\\nüìä Processing Summary:\")\nprint(f\"  ‚Ä¢ Total processed: {processed_count} chunks\")\nprint(f\"  ‚Ä¢ Errors: {error_count}\")\nprint(f\"  ‚Ä¢ Total time: {total_time:.1f} seconds\")\nprint(f\"  ‚Ä¢ Average rate: {processed_count/total_time:.2f} chunks/sec\")\nprint(f\"  ‚Ä¢ Documents in ChromaDB: {rag.collection.count()}\")\n\nprint(\"\\nüí° ChromaDB has automatically indexed all documents!\")\nprint(\"üí° Ready to answer questions!\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Test Basic RAG\n",
    "\n",
    "Test the basic RAG system with simple queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Test basic RAG with sample questions\n",
    "\n",
    "# Test with a simple question\n",
    "rag.ask(\"What are the main business activities of the companies?\", top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Hybrid Search Implementation\n",
    "\n",
    "Combines semantic search (ChromaDB) with keyword search for better accuracy.\n",
    "\n",
    "**Benefits:**\n",
    "- Catches exact keyword matches\n",
    "- Better handling of specific terms (company names, metrics)\n",
    "- 10-15% accuracy improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Hybrid Search Implementation - ChromaDB Version\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "class HybridSearch:\n",
    "    \"\"\"\n",
    "    Combine vector search (semantic) with keyword search (exact matches)\n",
    "    This improves accuracy by 10-15%\n",
    "    Now works with ChromaDB backend\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rag):\n",
    "        self.rag = rag\n",
    "\n",
    "    def keyword_search(self, query: str, top_k: int = 10):\n",
    "        \"\"\"\n",
    "        Simple keyword search using TF-IDF-like scoring\n",
    "\n",
    "        Args:\n",
    "            query: Search query\n",
    "            top_k: Number of results\n",
    "\n",
    "        Returns:\n",
    "            List of (chunk_index, score) tuples\n",
    "        \"\"\"\n",
    "        # Extract keywords from query\n",
    "        query_terms = set(re.findall(r'\\b\\w+\\b', query.lower()))\n",
    "\n",
    "        # Remove common words\n",
    "        stopwords = {'the', 'a', 'an', 'in', 'on', 'at', 'for', 'to', 'of', 'and', 'or'}\n",
    "        query_terms = query_terms - stopwords\n",
    "\n",
    "        # Score each chunk\n",
    "        scores = []\n",
    "        for idx, chunk in enumerate(self.rag.chunks):\n",
    "            chunk_terms = set(re.findall(r'\\b\\w+\\b', chunk.lower()))\n",
    "\n",
    "            # Count matching terms\n",
    "            matches = query_terms & chunk_terms\n",
    "\n",
    "            if matches:\n",
    "                # Simple scoring: number of matching terms\n",
    "                score = len(matches)\n",
    "\n",
    "                # Boost for exact phrase matches\n",
    "                if query.lower() in chunk.lower():\n",
    "                    score *= 2\n",
    "\n",
    "                scores.append((idx, score))\n",
    "\n",
    "        # Sort by score\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return scores[:top_k]\n",
    "\n",
    "    def hybrid_search(self, query: str, top_k: int = 5, alpha: float = 0.7):\n",
    "        \"\"\"\n",
    "        Combine vector search and keyword search\n",
    "\n",
    "        Args:\n",
    "            query: Search query\n",
    "            top_k: Number of results to return\n",
    "            alpha: Weight for vector search (1-alpha for keyword search)\n",
    "\n",
    "        Returns:\n",
    "            List of chunk indices\n",
    "        \"\"\"\n",
    "        # Vector search using ChromaDB\n",
    "        q_embedding = self.rag.embedder.encode([query], device='cuda')\n",
    "\n",
    "        # Query ChromaDB for more candidates\n",
    "        results = self.rag.collection.query(\n",
    "            query_embeddings=q_embedding.tolist(),\n",
    "            n_results=min(top_k * 2, len(self.rag.chunks))  # Get more candidates\n",
    "        )\n",
    "\n",
    "        # Get the IDs and convert to indices\n",
    "        vector_ids = results['ids'][0]\n",
    "        distances = results['distances'][0]\n",
    "\n",
    "        # Map IDs back to indices in self.rag.chunks\n",
    "        # ChromaDB returns IDs, we need to find corresponding indices\n",
    "        id_to_index = {}\n",
    "\n",
    "        if self.rag.collection.count() > 0:\n",
    "            all_results = self.rag.collection.get()\n",
    "            all_ids = all_results['ids']\n",
    "\n",
    "            for idx, doc_id in enumerate(all_ids):\n",
    "                id_to_index[doc_id] = idx\n",
    "\n",
    "        vector_indices = [id_to_index[doc_id] for doc_id in vector_ids if doc_id in id_to_index]\n",
    "\n",
    "        # Keyword search\n",
    "        keyword_results = self.keyword_search(query, top_k * 2)\n",
    "\n",
    "        # Combine scores\n",
    "        combined_scores = {}\n",
    "\n",
    "        # Add vector search scores (convert distance to similarity)\n",
    "        for i, idx in enumerate(vector_indices):\n",
    "            # Lower distance = better match\n",
    "            score = 1.0 / (1.0 + distances[i])\n",
    "            combined_scores[idx] = alpha * score\n",
    "\n",
    "        # Add keyword search scores (normalized)\n",
    "        if keyword_results:\n",
    "            max_keyword_score = max(score for _, score in keyword_results)\n",
    "            for idx, score in keyword_results:\n",
    "                normalized_score = score / max_keyword_score\n",
    "                if idx in combined_scores:\n",
    "                    combined_scores[idx] += (1 - alpha) * normalized_score\n",
    "                else:\n",
    "                    combined_scores[idx] = (1 - alpha) * normalized_score\n",
    "\n",
    "        # Sort by combined score\n",
    "        sorted_indices = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Return top-k\n",
    "        return [idx for idx, _ in sorted_indices[:top_k]]\n",
    "\n",
    "    def ask_hybrid(self, question: str, top_k: int = 5):\n",
    "        \"\"\"\n",
    "        Ask question using hybrid search\n",
    "\n",
    "        Args:\n",
    "            question: Question to ask\n",
    "            top_k: Number of chunks to retrieve\n",
    "\n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        if self.rag.collection.count() == 0:\n",
    "            print(\"‚ùå Please load documents first\")\n",
    "            return None\n",
    "\n",
    "        print(f\"‚ùì Question: {question}\\n\")\n",
    "        print(\"  üîç Using HYBRID search (vector + keyword)...\")\n",
    "\n",
    "        # Get relevant chunks using hybrid search\n",
    "        indices = self.hybrid_search(question, top_k)\n",
    "\n",
    "        # Build context\n",
    "        context_parts = []\n",
    "        sources_used = []\n",
    "\n",
    "        for i, idx in enumerate(indices):\n",
    "            chunk = self.rag.chunks[idx]\n",
    "            meta = self.rag.chunk_metadata[idx]\n",
    "\n",
    "            source_info = f\"{meta['company']} | {meta['section']}\"\n",
    "            sources_used.append(source_info)\n",
    "\n",
    "            context_parts.append(f\"[Source {i+1}: {source_info}]\\n{chunk}\")\n",
    "\n",
    "        context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "\n",
    "        # Generate answer (same as before)\n",
    "        prompt = f\"\"\"You are an expert financial analyst with deep knowledge of SEC filings and financial statements.\n",
    "\n",
    "Context from financial documents:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "1. Answer ONLY using information from the context above\n",
    "2. Think step-by-step if calculations are needed\n",
    "3. Cite which source (company and section) you're using\n",
    "4. Show your work for any calculations\n",
    "5. Be precise with numbers and include units\n",
    "6. If information is not in the context, say \"Information not available in provided documents\"\n",
    "\n",
    "Your analysis:\"\"\"\n",
    "\n",
    "        print(\"  ü§î Generating answer with GPT-3.5-turbo...\")\n",
    "\n",
    "        try:\n",
    "            response = self.rag.client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert financial analyst.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.3,\n",
    "                max_tokens=800\n",
    "            )\n",
    "\n",
    "            answer = response.choices[0].message.content\n",
    "\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"üìä ANSWER (using Hybrid Search)\")\n",
    "            print(\"=\"*70)\n",
    "            print(answer)\n",
    "            print(\"=\"*70)\n",
    "\n",
    "            print(\"\\nüìö Sources Used:\")\n",
    "            for i, source in enumerate(set(sources_used), 1):\n",
    "                print(f\"  {i}. {source}\")\n",
    "            print()\n",
    "\n",
    "            return answer\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            return None\n",
    "\n",
    "# Initialize hybrid search\n",
    "hybrid = HybridSearch(rag)\n",
    "\n",
    "print(\"‚úÖ Hybrid Search Implemented (ChromaDB)!\")\n",
    "print(\"üí° Usage: hybrid.ask_hybrid('your question')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Few-Shot Prompting\n",
    "\n",
    "Improves answer quality by showing the model examples of good financial analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Few-Shot Prompting\n",
    "\n",
    "class FewShotRAG:\n",
    "    \"\"\"\n",
    "    Add few-shot examples to improve accuracy\n",
    "    Shows the model examples of good answers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rag, hybrid_search):\n",
    "        self.rag = rag\n",
    "        self.hybrid = hybrid_search\n",
    "\n",
    "        # Define few-shot examples\n",
    "        self.examples = [\n",
    "            {\n",
    "                \"question\": \"What was Apple's revenue?\",\n",
    "                \"context\": \"Apple Inc. reported total revenue of $394 billion for fiscal 2023, representing a 15% increase year-over-year.\",\n",
    "                \"answer\": \"Based on the financial data from Apple Inc.'s fiscal 2023 filing, the company reported total revenue of $394 billion, which represents a 15% increase compared to the previous year.\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"What are the main risk factors?\",\n",
    "                \"context\": \"Risk Factors: Competition in cloud services is intense. Cybersecurity incidents could harm reputation. Economic uncertainty may reduce IT spending.\",\n",
    "                \"answer\": \"The main risk factors identified are: 1) Intense competition in cloud services, 2) Potential cybersecurity incidents that could damage reputation and financial results, and 3) Economic uncertainty that may lead to reduced IT spending by customers.\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"Compare gross margins\",\n",
    "                \"context\": \"Company A gross margin: 43.5%. Company B gross margin: 42.0%. Company C gross margin: 18.2%.\",\n",
    "                \"answer\": \"Comparing gross margins: Company A has the highest at 43.5%, followed by Company B at 42.0%, and Company C at 18.2%. Company A's margin is 1.5 percentage points higher than Company B and 25.3 percentage points higher than Company C.\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    def build_few_shot_prompt(self, question: str, context: str):\n",
    "        \"\"\"Build prompt with few-shot examples\"\"\"\n",
    "\n",
    "        prompt = \"You are an expert financial analyst. Here are examples of good analyses:\\n\\n\"\n",
    "\n",
    "        # Add examples\n",
    "        for i, example in enumerate(self.examples, 1):\n",
    "            prompt += f\"Example {i}:\\n\"\n",
    "            prompt += f\"Context: {example['context']}\\n\"\n",
    "            prompt += f\"Question: {example['question']}\\n\"\n",
    "            prompt += f\"Answer: {example['answer']}\\n\\n\"\n",
    "\n",
    "        # Add actual question\n",
    "        prompt += \"Now answer this question in the same style:\\n\\n\"\n",
    "        prompt += f\"Context from financial documents:\\n{context}\\n\\n\"\n",
    "        prompt += f\"Question: {question}\\n\\n\"\n",
    "        prompt += \"Instructions:\\n\"\n",
    "        prompt += \"1. Answer ONLY using information from the context\\n\"\n",
    "        prompt += \"2. Be specific with numbers and cite sources\\n\"\n",
    "        prompt += \"3. Show calculations step-by-step if needed\\n\"\n",
    "        prompt += \"4. Format your answer clearly\\n\\n\"\n",
    "        prompt += \"Your analysis:\"\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def ask_with_examples(self, question: str, top_k: int = 5):\n",
    "        \"\"\"Ask question using few-shot prompting\"\"\"\n",
    "\n",
    "        print(f\"‚ùì Question: {question}\\n\")\n",
    "        print(\"  üîç Searching with hybrid search + few-shot learning...\")\n",
    "\n",
    "        # Get context using hybrid search\n",
    "        indices = self.hybrid.hybrid_search(question, top_k)\n",
    "\n",
    "        context_parts = []\n",
    "        sources_used = []\n",
    "\n",
    "        for i, idx in enumerate(indices):\n",
    "            chunk = self.rag.chunks[idx]\n",
    "            meta = self.rag.chunk_metadata[idx]\n",
    "\n",
    "            source_info = f\"{meta['company']} | {meta['section']}\"\n",
    "            sources_used.append(source_info)\n",
    "\n",
    "            context_parts.append(f\"[Source {i+1}: {source_info}]\\n{chunk}\")\n",
    "\n",
    "        context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "\n",
    "        # Build few-shot prompt\n",
    "        prompt = self.build_few_shot_prompt(question, context)\n",
    "\n",
    "        print(\"  ü§î Generating answer with few-shot examples...\")\n",
    "\n",
    "        try:\n",
    "            response = self.rag.client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert financial analyst. Follow the example format exactly.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.2,  # Lower temperature for more consistent format\n",
    "                max_tokens=800\n",
    "            )\n",
    "\n",
    "            answer = response.choices[0].message.content\n",
    "\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"üìä ANSWER (with Few-Shot Learning)\")\n",
    "            print(\"=\"*70)\n",
    "            print(answer)\n",
    "            print(\"=\"*70)\n",
    "\n",
    "            print(\"\\nüìö Sources Used:\")\n",
    "            for i, source in enumerate(set(sources_used), 1):\n",
    "                print(f\"  {i}. {source}\")\n",
    "            print()\n",
    "\n",
    "            return answer\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            return None\n",
    "\n",
    "# Initialize few-shot RAG\n",
    "fewshot = FewShotRAG(rag, hybrid)\n",
    "\n",
    "print(\"‚úÖ Few-Shot Learning Implemented!\")\n",
    "print(\"üí° Usage: fewshot.ask_with_examples('your question')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ôªÔ∏è Cross-Encoder Re-Ranking\n",
    "\n",
    "Uses a cross-encoder to re-rank retrieved chunks for maximum relevance.\n",
    "\n",
    "**Process:**\n",
    "1. Retrieve top-20 candidates with hybrid search\n",
    "2. Score each candidate with cross-encoder\n",
    "3. Select top-5 highest-scored chunks\n",
    "4. Generate answer with best chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Cross-Encoder Re-Ranking\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "class ReRanker:\n",
    "    \"\"\"\n",
    "    Re-rank retrieved chunks using a cross-encoder\n",
    "    This improves accuracy by 5-10%\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rag, hybrid_search):\n",
    "        self.rag = rag\n",
    "        self.hybrid = hybrid_search\n",
    "\n",
    "        print(\"üì• Loading cross-encoder for re-ranking...\")\n",
    "        # Use a cross-encoder fine-tuned for semantic similarity\n",
    "        self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "        print(\"‚úÖ Cross-encoder loaded!\")\n",
    "\n",
    "    def rerank(self, query: str, candidate_indices: list):\n",
    "        \"\"\"\n",
    "        Re-rank candidates using cross-encoder\n",
    "\n",
    "        Args:\n",
    "            query: Search query\n",
    "            candidate_indices: List of chunk indices to re-rank\n",
    "\n",
    "        Returns:\n",
    "            Re-ranked list of indices\n",
    "        \"\"\"\n",
    "        # Get chunks\n",
    "        candidates = [self.rag.chunks[idx] for idx in candidate_indices]\n",
    "\n",
    "        # Score with cross-encoder\n",
    "        pairs = [[query, chunk] for chunk in candidates]\n",
    "        scores = self.reranker.predict(pairs)\n",
    "\n",
    "        # Sort by score\n",
    "        scored_indices = list(zip(candidate_indices, scores))\n",
    "        scored_indices.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return [idx for idx, _ in scored_indices]\n",
    "\n",
    "    def ask_with_reranking(self, question: str, retrieve_k: int = 20, final_k: int = 5):\n",
    "        \"\"\"\n",
    "        Ask question with retrieval + re-ranking\n",
    "\n",
    "        Args:\n",
    "            question: Question to ask\n",
    "            retrieve_k: Number of chunks to retrieve initially\n",
    "            final_k: Number of chunks to use after re-ranking\n",
    "\n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        print(f\"‚ùì Question: {question}\\n\")\n",
    "        print(f\"  üîç Step 1: Retrieving top {retrieve_k} candidates...\")\n",
    "\n",
    "        # Step 1: Get candidates with hybrid search\n",
    "        candidate_indices = self.hybrid.hybrid_search(question, retrieve_k)\n",
    "\n",
    "        print(f\"  ‚ôªÔ∏è  Step 2: Re-ranking to find best {final_k}...\")\n",
    "\n",
    "        # Step 2: Re-rank\n",
    "        reranked_indices = self.rerank(question, candidate_indices)[:final_k]\n",
    "\n",
    "        print(f\"  ‚úÖ Selected {final_k} most relevant chunks\\n\")\n",
    "\n",
    "        # Build context\n",
    "        context_parts = []\n",
    "        sources_used = []\n",
    "\n",
    "        for i, idx in enumerate(reranked_indices):\n",
    "            chunk = self.rag.chunks[idx]\n",
    "            meta = self.rag.chunk_metadata[idx]\n",
    "\n",
    "            source_info = f\"{meta['company']} | {meta['section']}\"\n",
    "            sources_used.append(source_info)\n",
    "\n",
    "            context_parts.append(f\"[Source {i+1}: {source_info}]\\n{chunk}\")\n",
    "\n",
    "        context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "\n",
    "        # Generate answer\n",
    "        prompt = f\"\"\"You are an expert financial analyst with deep knowledge of SEC filings and financial statements.\n",
    "\n",
    "Context from financial documents (re-ranked for relevance):\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "1. Answer ONLY using information from the context above\n",
    "2. Think step-by-step if calculations are needed\n",
    "3. Cite which source (company and section) you're using\n",
    "4. Show your work for any calculations\n",
    "5. Be precise with numbers and include units\n",
    "6. If information is not in the context, say \"Information not available in provided documents\"\n",
    "\n",
    "Your analysis:\"\"\"\n",
    "\n",
    "        print(\"  ü§î Generating answer...\")\n",
    "\n",
    "        try:\n",
    "            response = self.rag.client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert financial analyst.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.3,\n",
    "                max_tokens=800\n",
    "            )\n",
    "\n",
    "            answer = response.choices[0].message.content\n",
    "\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"üìä ANSWER (with Re-Ranking)\")\n",
    "            print(\"=\"*70)\n",
    "            print(answer)\n",
    "            print(\"=\"*70)\n",
    "\n",
    "            print(\"\\nüìö Sources Used:\")\n",
    "            for i, source in enumerate(set(sources_used), 1):\n",
    "                print(f\"  {i}. {source}\")\n",
    "            print()\n",
    "\n",
    "            return answer\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            return None\n",
    "\n",
    "# Initialize re-ranker\n",
    "reranker = ReRanker(rag, hybrid)\n",
    "\n",
    "print(\"‚úÖ Re-Ranking Implemented!\")\n",
    "print(\"üí° Usage: reranker.ask_with_reranking('your question')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Compare All Methods\n",
    "\n",
    "Test all implemented methods side-by-side with the same question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Compare all methods\n",
    "\n",
    "import time\n",
    "\n",
    "def compare_methods(question: str):\n",
    "    \"\"\"Compare all RAG methods with the same question\"\"\"\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(f\"   COMPARING ALL METHODS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nQuestion: {question}\\n\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    methods = [\n",
    "        (\"Basic RAG\", lambda: rag.ask(question, top_k=5)),\n",
    "        (\"Hybrid Search\", lambda: hybrid.ask_hybrid(question, top_k=5)),\n",
    "        (\"Few-Shot Learning\", lambda: fewshot.ask_with_examples(question, top_k=5)),\n",
    "        (\"Re-Ranking\", lambda: reranker.ask_with_reranking(question, retrieve_k=20, final_k=5))\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for name, method in methods:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"   METHOD: {name}\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "\n",
    "        start = time.time()\n",
    "        answer = method()\n",
    "        elapsed = time.time() - start\n",
    "\n",
    "        results[name] = {\n",
    "            'answer': answer,\n",
    "            'time': elapsed\n",
    "        }\n",
    "\n",
    "        print(f\"\\n‚è±Ô∏è  Time taken: {elapsed:.2f} seconds\")\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"   PERFORMANCE SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    for name, data in results.items():\n",
    "        print(f\"{name:25s} - {data['time']:.2f}s\")\n",
    "\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "# results = compare_methods(\"What are the main business activities described in these filings?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Test Queries\n",
    "\n",
    "Run various test queries to evaluate the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Test various queries\n",
    "\n",
    "# Test questions you can try:\n",
    "test_questions = [\n",
    "    \"What are the main business activities of the companies?\",\n",
    "    \"What are the key risk factors mentioned?\",\n",
    "    \"What financial metrics are discussed?\",\n",
    "    \"Compare the business strategies of different companies\",\n",
    "    \"What are the main revenue sources?\"\n",
    "]\n",
    "\n",
    "print(\"üìù Suggested test questions:\")\n",
    "print()\n",
    "for i, q in enumerate(test_questions, 1):\n",
    "    print(f\"{i}. {q}\")\n",
    "print()\n",
    "print(\"üí° Use: rag.ask('your question')\")\n",
    "print(\"üí° Or: hybrid.ask_hybrid('your question')\")\n",
    "print(\"üí° Or: fewshot.ask_with_examples('your question')\")\n",
    "print(\"üí° Or: reranker.ask_with_reranking('your question')\")\n",
    "print(\"üí° Or: compare_methods('your question') to test all methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä ChromaDB Statistics\n",
    "\n",
    "View statistics and information about the ChromaDB collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: View ChromaDB statistics\n",
    "\n",
    "def show_chromadb_stats():\n",
    "    \"\"\"Display detailed ChromaDB statistics\"\"\"\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(\"   CHROMADB STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    print(f\"\\nTotal chunks in database: {rag.collection.count()}\")\n",
    "    print(f\"Embedding dimension: {rag.embedding_dim}\")\n",
    "    print(f\"Collection name: {rag.collection.name}\")\n",
    "\n",
    "    # Get unique companies\n",
    "    if rag.collection.count() > 0:\n",
    "        results = rag.collection.get()\n",
    "        companies = set(meta['company'] for meta in results['metadatas'])\n",
    "\n",
    "        print(f\"\\nNumber of companies: {len(companies)}\")\n",
    "        print(\"\\nCompanies in database:\")\n",
    "\n",
    "        for company in sorted(companies):\n",
    "            # Count chunks per company\n",
    "            company_chunks = sum(1 for m in results['metadatas'] if m['company'] == company)\n",
    "            print(f\"  ‚Ä¢ {company}: {company_chunks} chunks\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "show_chromadb_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}